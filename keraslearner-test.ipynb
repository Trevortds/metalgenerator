{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import numpy as np\n",
    "\n",
    "# I'm making this from Trung Tran's LSTM tutorial. \n",
    "# I'm going to try to annotate it in my own words so that I can understand what's happening\n",
    "# And then change it to do what I want. \n",
    "\n",
    "data = ['i', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'd', 'r', 'e', 'a', 'm']\n",
    "\n",
    "# features. This will eventually be words\n",
    "chars = list(set(data))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# conversion to numbers. This is actually a really clever solution\n",
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "# setting up parameters. \n",
    "num_features = len(chars)\n",
    "# length of the group of words that the lstm will be shown at a time\n",
    "len_sequence = 3\n",
    "num_sequences = len(data)//len_sequence\n",
    "\n",
    "\n",
    "# need input and output tapes for the LSTM\n",
    "X = np.zeros((len(data)//len_sequence, len_sequence, num_features))\n",
    "y = np.zeros((len(data)//len_sequence, len_sequence, num_features))\n",
    "\n",
    "# for each of the sequences\n",
    "for i in range(0, len(data)//len_sequence):\n",
    "    # select the characters in the data that correspond to this sequence\n",
    "    X_sequence = data[i*len_sequence:(i+1)*len_sequence]\n",
    "    # convert to numeric\n",
    "    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "    #initialize \n",
    "    input_sequence = np.zeros((len_sequence, num_features))\n",
    "    for j in range(len_sequence):\n",
    "        #make a 1-hot vector for each of the letters the lstm is being shown\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1\n",
    "\n",
    "    X[i] = input_sequence\n",
    "    \n",
    "    # select targets: the symbol that follows\n",
    "    y_sequence = data[i*len_sequence+1:(i+1)*len_sequence+1]\n",
    "    # convert to numeric\n",
    "    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((len_sequence, num_features))\n",
    "    for j in range(len_sequence):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1\n",
    "        \n",
    "    y[i] = target_sequence\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize a sequential network\n",
    "model = Sequential()\n",
    "\n",
    "hidden_dim = 500\n",
    "layer_num = 2\n",
    "\n",
    "# add an initial lstm layer\n",
    "# I don't know why the input shape needs to be a tuple\n",
    "# the return-sequences parameter makes it give you multiple outputs\n",
    "model.add(LSTM(hidden_dim, input_shape=(None, num_features), return_sequences=True))\n",
    "\n",
    "# add more layers\n",
    "# I don't really know what adding a layer to an LSTM means, \n",
    "#  they're only ever shown with one.\n",
    "\n",
    "for i in range(layer_num-1):\n",
    "    model.add(LSTM(hidden_dim, return_sequences=True))\n",
    "\n",
    "# I don't see why the dense layer is necessary, but Tran says it is.\n",
    "# and in order to get the dense layer to work, a time distributed layer\n",
    "#  needs to go between.\n",
    "\n",
    "model.add(TimeDistributed(Dense(num_features)))\n",
    "\n",
    "# pick an activation for this layer\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# pick a loss function and optimization method. \n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# begin with some random characters and predict the next n characters\n",
    "\n",
    "def generate_text(model, length):\n",
    "    # generate a number and associated character\n",
    "    ix = [np.random.randint(num_features)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    # annoyingly, the big matrix of character sequences is called X\n",
    "    X = np.zeros((1, length, num_features))\n",
    "    \n",
    "    for i in range(length):\n",
    "        # for n characters\n",
    "        # update the big matrix with the last prediction\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        # print the last prediction to the command line\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        # get a new prediction\n",
    "        # I don't know what most of these arguments are. \n",
    "        # I don't understand why the prediction needs to be subscripted\n",
    "        # I guess the 1 means that you only get one output from argmax? \n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        \n",
    "        # convert to character and append to array\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    \n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I don't know if nb is supposed to mean something. This is just a counter\n",
    "\n",
    "nb_epoch = 0\n",
    "batch_size = 50\n",
    "generate_length = 50\n",
    "\n",
    "while True:\n",
    "    print(\"\\n\\n\")\n",
    "    # fit the model for one epoch\n",
    "    model.fit(X, y, batch_size=batch_size, verbose=1, nb_epoch=10)\n",
    "    # increment counter\n",
    "    nb_epoch += 1\n",
    "    # every epoch, show some text examples.\n",
    "    # this is a function defined below.\n",
    "#     generate_text(model, generate_length)\n",
    "    \n",
    "    if nb_epoch % 10 == 0:\n",
    "        # save every tenth epoch\n",
    "        print(\"epoch # {}\".format(nb_epoch*10))\n",
    "        generate_text(model, generate_length)\n",
    "        model.save_weights('checkpoint_{}_epoch{}.hdf5'.format(hidden_dim, nb_epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
